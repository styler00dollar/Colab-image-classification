{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-image-classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "E1icslyDumA6",
        "MetyROzzYhyi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbzGs1NUcEig"
      },
      "source": [
        "# Colab-image-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL0ZJvp6MLSh"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMw3aXKaGO0n",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "!pip install kornia efficientnet_pytorch x_transformers vit-pytorch swin-transformer-pytorch adamp tensorboardX torchvision timm madgrad\n",
        "!pip install git+https://github.com/styler00dollar/pytorch-lightning.git@fc86f4ca817d5ba1702a210a898ac2729c870112\n",
        "!git clone https://github.com/styler00dollar/Colab-image-classification\n",
        "%cd /content/Colab-image-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mxWmKkLxi-X"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWLGPAddmcQ6"
      },
      "source": [
        "%cd /content/Colab-image-classification\n",
        "!python means_stds.py --train_dir '/content/data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7jfkdu88kDQ"
      },
      "source": [
        "import timm\n",
        "from pprint import pprint\n",
        "model_names = timm.list_models(pretrained=True)\n",
        "pprint(model_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqXK0H-8H6LT"
      },
      "source": [
        "# delete ipynb_checkpoints file if needed, since dataloader does not a filetype check\n",
        "!sudo find /content/ -iname \"*ipynb_checkpoints*\"\n",
        "!sudo rm -rf /content/data/.ipynb_checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmmcBR6kw970",
        "cellView": "form"
      },
      "source": [
        "#@title config.yaml\n",
        "%%writefile /content/Colab-image-classification/config.yaml\n",
        "print_training_epoch_end_metrics: False # RAM requirements for big datasets are high, disble it if you have big datasets. does not imply validation\n",
        "use_amp: False\n",
        "gpus: 0\n",
        "use_swa: False\n",
        "save_step_frequency: 5000\n",
        "\n",
        "# calculate these with means_stds.py!\n",
        "means: [0.7032, 0.6346, 0.6234]\n",
        "std: [0.2520, 0.2507, 0.2417]\n",
        "\n",
        "optimizer: AdamP # Adam | AdamP | SGDP | MADGRAD (pip install madgrad)\n",
        "lr: 0.0001\n",
        "batch_size: 2 # 62\n",
        "aug: MuAugment # gridmix, cutmix, MuAugment, None\n",
        "loss: normal # normal | CenterLoss\n",
        "size: 256 # image size\n",
        "precision: 32 # 32\n",
        "max_epochs: 9999\n",
        "progress_bar_refresh_rate: 20\n",
        "default_root_dir: \"/content/\"\n",
        "\n",
        "path:\n",
        "  # pretrain\n",
        "  pretrain:\n",
        "  checkpoint_path: \n",
        "\n",
        "  training_path: '/content/data/' # input folder\n",
        "  validation_path: '/content/data/' # validation folder\n",
        "  test_path: '/content/data/' # validation folder / test.py\n",
        "  log_path: \"/content/logs/\"\n",
        "  checkpoint_save_path: \"/content/\" \n",
        "\n",
        "\n",
        "# which model architecture to train\n",
        "\n",
        "# efficientnet-b0 up to efficientnet-b8 (pip install efficientnet_pytorch)\n",
        "# mobilenetv3_small / mobilenetv3_large\n",
        "# resnet50 / resnet101 / resnet152\n",
        "# ViT / DeepViT\n",
        "# RepVGG-A0, RepVGG-A1, RepVGG-A2, RepVGG-B0, RepVGG-B1, RepVGG-B1g2, RepVGG-B1g4, RepVGG-B2, RepVGG-B2g2, RepVGG-B2g4, RepVGG-B3, RepVGG-B3g2, RepVGG-B3g4\n",
        "# squeezenet_1_0 / squeezenet_1_1\n",
        "# vgg11, vgg13, vgg16, vgg19\n",
        "# SwinTransformer\n",
        "\n",
        "# for vgg and resnet\n",
        "#pretrain: False\n",
        "\n",
        "# timm\n",
        "# pip install timm\n",
        "# you can loop up models here: https://rwightman.github.io/pytorch-image-models/\n",
        "# Example: \"tf_efficientnetv2_s\"\n",
        "#model_train: timm\n",
        "#model_choise: 'tf_efficientnetv2_b0'\n",
        "#model_choise: \"swin_tiny_patch4_window7_224\"\n",
        "\n",
        "#################\n",
        "\n",
        "# other models with configurable parameters\n",
        "#model_train: effV2\n",
        "#conv: fft # fft | conv2d\n",
        "#model_size: s # s | m | l | xl\n",
        "\n",
        "# x-transformers\n",
        "# pip install x-transformers\n",
        "#model_train: x_transformers\n",
        "#image_size: 512\n",
        "#patch_size: 32\n",
        "#dim: 512\n",
        "#depth: 6\n",
        "#heads: 8\n",
        "\n",
        "# Warning: only 256px input (b110)\n",
        "#model_train: mobilevit \n",
        "#model_size: xxs # xxs | xs | s\n",
        "\n",
        "# because of too many parameters, a seperate config file named \"hrt_config.yaml\" is available\n",
        "#model_train: hrt\n",
        "\n",
        "# volo (2021) (224px)\n",
        "#model_train: volo\n",
        "#model_size: volo_d1 # volo_d1 up to volo_d5\n",
        "#pretrain: False\n",
        "\n",
        "# pvt_v2 (2021)\n",
        "#model_train: pvt_v2\n",
        "#model_size: pvt_v2_b0 # pvt_v2_b0 to pvt_v2_b5, pvt_v2_b2_li\n",
        "#pretrain: False\n",
        "\n",
        "# ConvMLP (2021)\n",
        "#model_train: ConvMLP\n",
        "#model_size: convmlp_s # convmlp_s | convmlp_m | convmlp_l\n",
        "#pretrain: False\n",
        "\n",
        "# FocalTransformer (2021)\n",
        "#model_train: FocalTransformer\n",
        "\n",
        "# MobileFormer (2021)\n",
        "#model_train: mobile_former\n",
        "#model_size: config_52 # config_52 | config_294 | config_508\n",
        "\n",
        "# poolformer (2021)\n",
        "model_train: poolformer\n",
        "model_size: poolformer_s12 # poolformer_s12 | poolformer_s24 | poolformer_s36 | poolformer_m36 | poolformer_m48\n",
        "\n",
        "#################\n",
        "\n",
        "num_classes: 2 # Warning: Some do require amount classes + 1 as num_classes. \n",
        "diffaug_activate: False\n",
        "policy: 'color,translation' # [color,translation,cutout]\n",
        "num_workers: 12\n",
        "\n",
        "################################################################\n",
        "# inference\n",
        "# args for sort.py / test.py (only supports efficientnet)\n",
        "resize_method: PIL # PIL | OpenCV\n",
        "model_path: '/content/model.pth'\n",
        "path0: '/content/0'\n",
        "path1: '/content/1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2iHlBBIDCnu"
      },
      "source": [
        "%cd /content/Colab-image-classification\n",
        "!python train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFIdqXQBukCK"
      },
      "source": [
        "----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1icslyDumA6"
      },
      "source": [
        "# TensorRT\n",
        "\n",
        "Original Colab: [here](https://colab.research.google.com/drive/1oe_aflRfCwRehho_8QlD8YFKUQ_I5sA6#scrollTo=tgSrJdHic-Du)\n",
        "\n",
        "Colab-torch2trt: [styler00dollar/Colab-torch2trt](https://github.com/styler00dollar/Colab-torch2trt/blob/main/Colab-torch2trt.ipynb)\n",
        "\n",
        "onnx-tensorrt: [onnx/onnx-tensorrt](https://github.com/onnx/onnx-tensorrt)\n",
        "\n",
        "\n",
        "TensorRT gives better performance. You need to get 2 files.\n",
        "Currently, the cuda version inside Colab is 11.0, that's why you need to get:\n",
        "\n",
        "```\n",
        "nv-tensorrt-repo-ubuntu1804-cuda11.0-trt7.2.3.4-ga-20210226_1-1_amd64.deb\n",
        "\n",
        "and\n",
        " \n",
        "TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar\\TensorRT-7.2.3.4\\python\\tensorrt-7.2.3.4-cp37-none-linux_x86_64.whl (inside TensorRT-7.2.3.4.Ubuntu-18.04.x86_64-gnu.cuda-11.0.cudnn8.1.tar.gz)\n",
        "```\n",
        "\n",
        "You can download these files [here](https://developer.nvidia.com/nvidia-tensorrt-download). Warning: You need an account (which can be created for free).\n",
        "\n",
        "If you want to use other versions, you need to adjust the install script.\n",
        "\n",
        "Warning: Not everything works with tentorrt.\n",
        "- Not working: efficientnet\n",
        "- Working: Resnet\n",
        "- Everything else is untested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIDf38XUOFZe"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Ocaf070zOEzl"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "44mobXYlOH79"
      },
      "source": [
        "#@title install\n",
        "import os\n",
        "os.environ[\"os1\"]=\"ubuntu1804\"\n",
        "os.environ[\"tag\"]= \"cuda11.0-trt7.2.3.4-ga-20210226\" #@param\n",
        "os.environ[\"version\"]= \"7.2.3-1+cuda11.0\" #@param\n",
        "data_path = '/content/drive/MyDrive/tensorrt 11.0/' #@param\n",
        "os.chdir(data_path)\n",
        "!sudo dpkg -i nv-tensorrt-repo-${os1}-${tag}_1-1_amd64.deb\n",
        "!sudo apt-key add /var/nv-tensorrt-repo-${tag}/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install libnvinfer7=${version} libnvonnxparsers7=${version} libnvparsers7=${version} libnvinfer-plugin7=${version} libnvinfer-dev=${version} libnvonnxparsers-dev=${version} libnvparsers-dev=${version} libnvinfer-plugin-dev=${version} python-libnvinfer=${version} python3-libnvinfer=${version}\n",
        "!sudo apt-mark hold libnvinfer7 libnvonnxparsers7 libnvparsers7 libnvinfer-plugin7 libnvinfer-dev libnvonnxparsers-dev libnvparsers-dev libnvinfer-plugin-dev python-libnvinfer python3-libnvinfer\n",
        "!sudo apt-get install tensorrt=${version}\n",
        "!sudo apt-get install python3-libnvinfer-dev=${version}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70LQW-ZtOKUY"
      },
      "source": [
        "**Restart colab (Runtime > Restart Runtime)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0AcR335OJ1N"
      },
      "source": [
        "!pip install \"/content/drive/MyDrive/tensorrt 11.0/tensorrt-7.2.3.4-cp37-none-linux_x86_64.whl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6kSR9jp-WBw"
      },
      "source": [
        "!python onnx_convert.py --model_train 'resnet50' --num_classes 3 --output_path \"/content/output.onnx\" \\\n",
        "  --model_path '/content/Colab-image-classification/Checkpoint_1_7_loss_0.162406_acc_0.666667_D.pth'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPxo4HwyhwBb",
        "cellView": "form"
      },
      "source": [
        "#@title Install (pycuda, onnx, onnx-tensorrt)\n",
        "%cd /content/\n",
        "!pip install pycuda\n",
        "!pip install onnx\n",
        "#%cd /content/\n",
        "!git clone https://github.com/onnx/onnx-tensorrt\n",
        "%cd onnx-tensorrt\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NY16lw9RTeK",
        "cellView": "form"
      },
      "source": [
        "#@title Example usage of tensorrt backend\n",
        "import onnx\n",
        "import onnx_tensorrt.backend as backend\n",
        "import numpy as np\n",
        "onnx_path = \"/content/quant.onnx\" #@param\n",
        "model = onnx.load(onnx_path)\n",
        "engine = backend.prepare(model, device='CUDA:0', fp16_mode=True)\n",
        "input_data = np.random.random(size=(1, 3, 256, 256)).astype(np.float32)\n",
        "output_data = engine.run(input_data)[0]\n",
        "print(output_data)\n",
        "print(output_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RuCniSZtUao"
      },
      "source": [
        "%cd /content/Colab-image-classification\n",
        "# moving files with 2 classes example (onnx)\n",
        "!python sort_onnx.py --data_input_path \"/content/data/\" --onnx_path '/content/test.onnx' \\\n",
        "  --path0 '/content/0/' --path1 '/content/1/' --num_classes 3 --height_min 256 --width_min 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MetyROzzYhyi"
      },
      "source": [
        "# ONNX Quantization\n",
        "Maybe broken, use at your own risk.\n",
        "- Not working: EfficientNet, Resnet\n",
        "- Everything else is untested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqrp-XBxURWR"
      },
      "source": [
        "!pip install onnx -U\n",
        "!pip install onnxruntime-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ganKcMCuURWR"
      },
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_qat, QuantType\n",
        "\n",
        "model_fp32 = '/content/output.onnx'\n",
        "model_quant = '/content/quant.onnx'\n",
        "quantized_model = quantize_qat(model_fp32, model_quant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrKWMPkZm2sw"
      },
      "source": [
        "# Links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2-4OPgcm7fO"
      },
      "source": [
        "\n",
        "GridMixup: [IlyaDobrynin/GridMixup](https://github.com/IlyaDobrynin/GridMixup)\n",
        "\n",
        "Centerloss: [KaiyangZhou/pytorch-center-loss](https://github.com/KaiyangZhou/pytorch-center-loss)\n",
        "\n",
        "Cutmix: [hysts/pytorch_cutmix](https://github.com/hysts/pytorch_cutmix)\n",
        "\n",
        "Diffaug: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "AdamP: [clovaai/AdamP](https://github.com/clovaai/AdamP)\n",
        "\n",
        "EfficientNet repo: [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n",
        "\n",
        "RepVGG: [DingXiaoH/RepVGG](https://github.com/DingXiaoH/RepVGG)\n",
        "\n",
        "MobileNetV3: [kuan-wang/pytorch-mobilenet-v3](https://github.com/kuan-wang/pytorch-mobilenet-v3)\n",
        "\n",
        "swin-transformer: [berniwal/swin-transformer-pytorch](https://github.com/berniwal/swin-transformer-pytorch)\n",
        "\n",
        "Resnest: [zhanghang1989/ResNeSt](https://github.com/zhanghang1989/ResNeSt)\n",
        "\n",
        "timm: [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n",
        "\n",
        "madgrad: [facebookresearch/madgrad](https://github.com/facebookresearch/madgrad)\n",
        "\n",
        "EfficientNetV2: [efficientnetv2.pytorch](https://github.com/d-li14/efficientnetv2.pytorch)\n",
        "\n",
        "lama: [saic-mdal/lama](https://github.com/saic-mdal/lama)\n",
        "\n",
        "x-transformers: [lucidrains/x-transformers](https://github.com/lucidrains/x-transformers)\n",
        "\n",
        "mobilevit: [chinhsuanwu/mobilevit-pytorch](https://github.com/chinhsuanwu/mobilevit-pytorch)\n",
        "\n",
        "hrt: [HRNet/HRFormer](https://github.com/HRNet/HRFormer)\n",
        "\n",
        "MuarAugment: [adam-mehdi/MuarAugment](https://github.com/adam-mehdi/MuarAugment)\n",
        "\n",
        "poolformer: [sail-sg/poolformer](https://github.com/sail-sg/poolformer)\n",
        "\n",
        "Original repo: [bentrevett/pytorch-image-classification](https://github.com/bentrevett/pytorch-image-classification)\n",
        "\n",
        "My fork: [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification)"
      ]
    }
  ]
}