{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_EfficientNet_lightning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbzGs1NUcEig"
      },
      "source": [
        "# Colab-efficientnet-lightning\n",
        "\n",
        "GridMixup: [IlyaDobrynin/GridMixup](https://github.com/IlyaDobrynin/GridMixup)\n",
        "\n",
        "Diffaug: [mit-han-lab/data-efficient-gans](https://github.com/mit-han-lab/data-efficient-gans)\n",
        "\n",
        "AdamP: [clovaai/AdamP](https://github.com/clovaai/AdamP)\n",
        "\n",
        "EfficientNet repo: [lukemelas/EfficientNet-PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch)\n",
        "\n",
        "Original repo: [bentrevett/pytorch-image-classification](https://github.com/bentrevett/pytorch-image-classification)\n",
        "\n",
        "My fork: [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification)\n",
        "\n",
        "Currently trains without validation and displays training-only metrics. Saves model as pytorch ``pth``. Uses GridMix loss as default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL0ZJvp6MLSh"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMw3aXKaGO0n",
        "cellView": "form"
      },
      "source": [
        "#@title Install\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install adamp\n",
        "!pip install pytorch_lightning\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wHlVV9zMNqO",
        "cellView": "form"
      },
      "source": [
        "#@title print means and stds\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "train_dir = '/content/data/' #@param\n",
        "train_data = datasets.ImageFolder(root = train_dir, \n",
        "                                  transform = transforms.ToTensor())\n",
        "\n",
        "means = torch.zeros(3)\n",
        "stds = torch.zeros(3)\n",
        "\n",
        "for img, label in tqdm(train_data):\n",
        "    means += torch.mean(img, dim = (1,2))\n",
        "    stds += torch.std(img, dim = (1,2))\n",
        "\n",
        "means /= len(train_data)\n",
        "stds /= len(train_data)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Calculated means: {means}')\n",
        "print(f'Calculated stds: {stds}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "mBa_nlVdM_Yt"
      },
      "source": [
        "#@title init.py\n",
        "import torch.nn.init as init\n",
        "\n",
        "def weights_init(net, init_type = 'kaiming', init_gain = 0.02):\n",
        "    #Initialize network weights.\n",
        "    #Parameters:\n",
        "    #    net (network)       -- network to be initialized\n",
        "    #    init_type (str)     -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "    #    init_var (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "\n",
        "        if hasattr(m, 'weight') and classname.find('Conv') != -1:\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain = init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain = init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('Linear') != -1:\n",
        "            init.normal_(m.weight, 0, 0.01)\n",
        "            init.constant_(m.bias, 0)\n",
        "\n",
        "    # Apply the initialization function <init_func>\n",
        "    print('Initialization method [{:s}]'.format(init_type))\n",
        "    net.apply(init_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYS3iUfLNzV-",
        "cellView": "form"
      },
      "source": [
        "#@title dataloader.py\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "class DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, training_path: str = './', validation_path: str = './', test_path: str = './', batch_size: int = 5, num_workers: int = 2, size = 256):\n",
        "        super().__init__()\n",
        "        self.training_dir = training_path\n",
        "        self.validation_dir = validation_path\n",
        "        self.test_dir = test_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.size = size\n",
        "\n",
        "        self.means = [0.6013, 0.5443, 0.5521] #@param {type:\"raw\"}\n",
        "        self.std = [0.2496, 0.2509, 0.2433] #@param {type:\"raw\"}\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        img_tf = transforms.Compose([\n",
        "            transforms.Resize(size=self.size),\n",
        "            transforms.RandomRotation(5),\n",
        "            #transforms.CenterCrop(size=self.size),\n",
        "            transforms.RandomCrop(self.size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = self.means, \n",
        "                                  std = self.std)\n",
        "        ])\n",
        "        \n",
        "        self.DS_train = datasets.ImageFolder(root = self.training_dir, \n",
        "                                  transform = img_tf)\n",
        "        self.DS_validation = datasets.ImageFolder(root = self.training_dir, \n",
        "                                  transform = img_tf)\n",
        "        self.DS_test = datasets.ImageFolder(root = self.training_dir, \n",
        "                                  transform = img_tf)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.DS_train, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.DS_validation, batch_size=self.batch_size, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.DS_test, batch_size=self.batch_size, num_workers=self.num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzyNF8pqONEb",
        "cellView": "form"
      },
      "source": [
        "#@title GridMixupLoss.py\n",
        "\"\"\"\n",
        "gridmix_pytorch.py (27-3-20)\n",
        "https://github.com/IlyaDobrynin/GridMixup/blob/main/gridmix/gridmix_pytorch.py\n",
        "\"\"\"\n",
        "import typing as t\n",
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class GridMixupLoss(nn.Module):\n",
        "    \"\"\" Implementation of GridMixup loss\n",
        "\n",
        "    :param alpha: Percent of the first image on the crop. Can be float or Tuple[float, float]\n",
        "                    - if float: lambda parameter gets from the beta-distribution np.random.beta(alpha, alpha)\n",
        "                    - if Tuple[float, float]: lambda parameter gets from the uniform\n",
        "                        distribution np.random.uniform(alpha[0], alpha[1])\n",
        "    :param n_holes_x: Number of holes by OX\n",
        "    :param hole_aspect_ratio: hole aspect ratio\n",
        "    :param crop_area_ratio: Define percentage of the crop area\n",
        "    :param crop_aspect_ratio: Define crop aspect ratio\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            alpha: t.Union[float, t.Tuple[float, float]] = (0.1, 0.9),\n",
        "            n_holes_x: t.Union[int, t.Tuple[int, int]] = 20,\n",
        "            hole_aspect_ratio: t.Union[float, t.Tuple[float, float]] = 1.,\n",
        "            crop_area_ratio: t.Union[float, t.Tuple[float, float]] = 1.,\n",
        "            crop_aspect_ratio: t.Union[float, t.Tuple[float, float]] = 1.,\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.n_holes_x = n_holes_x\n",
        "        self.hole_aspect_ratio = hole_aspect_ratio\n",
        "        self.crop_area_ratio = crop_area_ratio\n",
        "        self.crop_aspect_ratio = crop_aspect_ratio\n",
        "        if isinstance(self.n_holes_x, int):\n",
        "            self.n_holes_x = (self.n_holes_x, self.n_holes_x)\n",
        "        if isinstance(self.hole_aspect_ratio, float):\n",
        "            self.hole_aspect_ratio = (self.hole_aspect_ratio, self.hole_aspect_ratio)\n",
        "        if isinstance(self.crop_area_ratio, float):\n",
        "            self.crop_area_ratio = (self.crop_area_ratio, self.crop_area_ratio)\n",
        "        if isinstance(self.crop_aspect_ratio, float):\n",
        "            self.crop_aspect_ratio = (self.crop_aspect_ratio, self.crop_aspect_ratio)\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"gridmixup\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_random_crop(height: int, width: int, crop_area_ratio: float, crop_aspect_ratio: float) -> t.Tuple:\n",
        "        crop_area = int(height * width * crop_area_ratio)\n",
        "        crop_width = int(np.sqrt(crop_area / crop_aspect_ratio))\n",
        "        crop_height = int(crop_width * crop_aspect_ratio)\n",
        "\n",
        "        cx = np.random.random()\n",
        "        cy = np.random.random()\n",
        "\n",
        "        y1 = int((height - crop_height) * cy)\n",
        "        y2 = y1 + crop_height\n",
        "        x1 = int((width - crop_width) * cx)\n",
        "        x2 = x1 + crop_width\n",
        "        return x1, y1, x2, y2\n",
        "\n",
        "    def _get_gridmask(\n",
        "            self,\n",
        "            image_shape: t.Tuple[int, int],\n",
        "            crop_area_ratio: float,\n",
        "            crop_aspect_ratio: float,\n",
        "            lam: float,\n",
        "            nx: int,\n",
        "            ar: float,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\" Method make grid mask\n",
        "\n",
        "        :param image_shape: Shape of the images\n",
        "        :param lam: Lambda parameter\n",
        "        :param crop_area_ratio: Ratio of the crop area\n",
        "        :param crop_aspect_ratio: Aspect ratio of the crop\n",
        "        :param nx: Amount of holes by width\n",
        "        :param ar: Aspect ratio of the hole\n",
        "        :return: Binary mask, where holes == 1, background == 0\n",
        "        \"\"\"\n",
        "        img_height, img_width = image_shape\n",
        "\n",
        "        # Get coordinates of random box\n",
        "        xc1, yc1, xc2, yc2 = self._get_random_crop(\n",
        "            height=img_height,\n",
        "            width=img_width,\n",
        "            crop_area_ratio=crop_area_ratio,\n",
        "            crop_aspect_ratio=crop_aspect_ratio\n",
        "        )\n",
        "        height = yc2 - yc1\n",
        "        width = xc2 - xc1\n",
        "\n",
        "        if not 1 <= nx <= width // 2:\n",
        "            raise ValueError(\n",
        "                f\"The nx must be between 1 and {width // 2}.\\n\"\n",
        "                f\"Give: {nx}\"\n",
        "            )\n",
        "\n",
        "        # Get patch width, height and ny\n",
        "        patch_width = math.ceil(width / nx)\n",
        "        patch_height = int(patch_width * ar)\n",
        "        ny = math.ceil(height / patch_height)\n",
        "\n",
        "        # Calculate ratio of the hole - percent of hole pixels in the patch\n",
        "        ratio = np.sqrt(1 - lam)\n",
        "\n",
        "        # Get hole size\n",
        "        hole_width = int(patch_width * ratio)\n",
        "        hole_height = int(patch_height * ratio)\n",
        "\n",
        "        # min 1 pixel and max patch length - 1\n",
        "        hole_width = min(max(hole_width, 1), patch_width - 1)\n",
        "        hole_height = min(max(hole_height, 1), patch_height - 1)\n",
        "\n",
        "        # Make grid mask\n",
        "        holes = []\n",
        "        for i in range(nx + 1):\n",
        "            for j in range(ny + 1):\n",
        "                x1 = min(patch_width * i, width)\n",
        "                y1 = min(patch_height * j, height)\n",
        "                x2 = min(x1 + hole_width, width)\n",
        "                y2 = min(y1 + hole_height, height)\n",
        "                holes.append((x1, y1, x2, y2))\n",
        "\n",
        "        mask = np.zeros(shape=image_shape, dtype=np.uint8)\n",
        "        for x1, y1, x2, y2 in holes:\n",
        "            mask[yc1+y1:yc1+y2, xc1+x1:xc1+x2] = 1\n",
        "        return mask\n",
        "\n",
        "    def get_sample(self, images: torch.Tensor, targets: torch.Tensor) -> t.Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Method returns augmented images and targets\n",
        "\n",
        "        :param images: Batch of non-augmented images\n",
        "        :param targets: Batch of non-augmented targets\n",
        "        :return: Augmented images and targets\n",
        "        \"\"\"\n",
        "        # Get new indices\n",
        "        indices = torch.randperm(images.size(0)).to(images.device)\n",
        "\n",
        "        # Shuffle labels\n",
        "        shuffled_targets = targets[indices].to(targets.device)\n",
        "\n",
        "        # Get image shape\n",
        "        height, width = images.shape[2:]\n",
        "\n",
        "        # Get lambda\n",
        "        if isinstance(self.alpha, float):\n",
        "            lam = np.random.beta(self.alpha, self.alpha)\n",
        "        else:\n",
        "            lam = np.random.uniform(self.alpha[0], self.alpha[1])\n",
        "\n",
        "        nx = random.randint(self.n_holes_x[0], self.n_holes_x[1])\n",
        "        ar = np.random.uniform(self.hole_aspect_ratio[0], self.hole_aspect_ratio[1])\n",
        "        crop_area_ratio = np.random.uniform(self.crop_area_ratio[0], self.crop_area_ratio[1])\n",
        "        crop_aspect_ratio = np.random.uniform(self.crop_aspect_ratio[0], self.crop_aspect_ratio[1])\n",
        "        mask = self._get_gridmask(\n",
        "            image_shape=(height, width),\n",
        "            crop_area_ratio=crop_area_ratio,\n",
        "            crop_aspect_ratio=crop_aspect_ratio,\n",
        "            lam=lam,\n",
        "            nx=nx,\n",
        "            ar=ar\n",
        "        )\n",
        "        # Adjust lambda to exactly match pixel ratio\n",
        "        lam = 1 - (mask.sum() / (images.size()[-1] * images.size()[-2]))\n",
        "\n",
        "        # Make shuffled images\n",
        "        mask = torch.from_numpy(mask).to(targets.device)\n",
        "        images = images * (1 - mask) + images[indices, ...] * mask\n",
        "\n",
        "        # Prepare out labels\n",
        "        lam_list = torch.from_numpy(np.ones(shape=targets.shape) * lam).to(targets.device)\n",
        "        out_targets = torch.cat([targets, shuffled_targets, lam_list], dim=1).transpose(0, 1)\n",
        "        return images, out_targets\n",
        "\n",
        "    def forward(self, preds: torch.Tensor, trues: torch.Tensor) -> torch.Tensor:\n",
        "        lam = trues[-1][0].float()\n",
        "        trues1, trues2 = trues[0].long(), trues[1].long()\n",
        "        loss = self.loss(preds, trues1) * lam + self.loss(preds, trues2) * (1 - lam)\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vYWcBQEocjqX"
      },
      "source": [
        "#@title diffaug.py\n",
        "\"\"\"\n",
        "DiffAugment_pytorch.py (27-3-20)\n",
        "https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py\n",
        "\"\"\"\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbfc8hP2drsK",
        "cellView": "form"
      },
      "source": [
        "#@title accuracy.py\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def calc_accuracy(preds: torch.Tensor, trues: torch.Tensor) -> float:\n",
        "    lam = trues[-1, :][0].data.cpu().numpy()\n",
        "    true_label = [trues[0, :].long(), trues[1, :].long()]\n",
        "    trues = true_label[0] if lam > 0.5 else true_label[1]\n",
        "    trues = trues.data.cpu().numpy().astype(np.uint8)\n",
        "    preds = torch.softmax(preds, dim=1).float()\n",
        "    preds = np.argmax(preds.data.cpu().numpy(), axis=1).astype(np.uint8)\n",
        "    metric = accuracy_score(trues, preds)\n",
        "    return float(metric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNk6tFD0MTFy",
        "cellView": "form"
      },
      "source": [
        "#@title CustomTrainClass.py\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from adamp import AdamP\n",
        "#from adamp import SGDP\n",
        "import numpy as np\n",
        "\n",
        "class CustomTrainClass(pl.LightningModule):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model_train = 'efficientnet-b0' #@param [\"efficientnet-b0\", \"efficientnet-b1\", \"efficientnet-b2\", \"efficientnet-b3\", \"efficientnet-b4\", \"efficientnet-b5\", \"efficientnet-b6\", \"efficientnet-b7\"] {type:\"string\"}\n",
        "    num_classes = 2 #@param\n",
        "    if model_train == 'efficientnet-b0':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b1':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b1', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b2':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b2', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b3':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b3', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b4':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b4', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b5':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b5', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b6':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b6', num_classes=num_classes)\n",
        "    elif model_train == 'efficientnet-b7':\n",
        "      self.netD = EfficientNet.from_pretrained('efficientnet-b7', num_classes=num_classes)\n",
        "\n",
        "    #weights_init(self.netD, 'kaiming') #only use this if there is no pretrain\n",
        "\n",
        "    self.criterion = GridMixupLoss(\n",
        "        alpha=(0.4, 0.7),\n",
        "        hole_aspect_ratio=1.,\n",
        "        crop_area_ratio=(0.5, 1),\n",
        "        crop_aspect_ratio=(0.5, 2),\n",
        "        n_holes_x=(2, 6)\n",
        "    )\n",
        "    self.accuracy = []\n",
        "    self.losses = []\n",
        "    self.diffaug_activate = False #@param\n",
        "\n",
        "    #@markdown Supports ``'color,translation,cutout'``\n",
        "    self.policy = 'color' #@param\n",
        "\n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "    inputs, targets = self.criterion.get_sample(images=train_batch[0], targets=train_batch[1].unsqueeze(-1))  \n",
        "    targets = targets -1 # fixing range\n",
        "\n",
        "    if self.diffaug_activate == False:\n",
        "      preds = self.netD(inputs)\n",
        "    else:\n",
        "      preds = self.netD(DiffAugment(inputs, policy=self.policy))\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = self.criterion(preds, targets) \n",
        "    \n",
        "    writer.add_scalar('loss', loss, self.current_epoch)\n",
        "\n",
        "    self.accuracy.append(calc_accuracy(preds, targets))\n",
        "    self.losses.append(loss.item())\n",
        "    return loss  \n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      #optimizer = torch.optim.Adam(self.netD.parameters(), lr=2e-3)\n",
        "      optimizer = AdamP(self.netD.parameters(), lr=2e-4, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "      #optimizer = SGDP(self.netD.parameters(), lr=0.1, weight_decay=1e-5, momentum=0.9, nesterov=True)\n",
        "      return optimizer\n",
        "\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "      loss_mean = np.mean(self.losses)\n",
        "      accuracy_mean = np.mean(self.accuracy)\n",
        "      print(f\"'Epoch': {self.current_epoch}, 'loss': {loss_mean}, 'accuracy': {accuracy_mean}\")\n",
        "      \n",
        "      # logging\n",
        "      self.log('loss_mean', loss_mean, prog_bar=True, logger=True, on_epoch=True)\n",
        "      self.log('accuracy_mean', accuracy_mean, prog_bar=True, logger=True, on_epoch=True)\n",
        "\n",
        "      self.losses = []\n",
        "      self.accuracy = []\n",
        "\n",
        "      torch.save(trainer.model.netD.state_dict(), f\"Checkpoint_{self.current_epoch}_{self.global_step}_loss_{loss_mean:3f}_acc_{accuracy_mean:3f}_D.pth\")\n",
        "\n",
        "  def validation_step(self, train_batch, train_idx):\n",
        "      print(\"not implemented\")\n",
        "  def test_step(self, train_batch, train_idx):\n",
        "      print(\"not implemented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MsvuUkxToFk",
        "cellView": "form"
      },
      "source": [
        "#@title training\n",
        "dm = DataModule(batch_size=2, training_path='/content/data/', num_workers = 1, size = 256)\n",
        "model = CustomTrainClass()\n",
        "# skipping validation with limit_val_batches=0\n",
        "trainer = pl.Trainer(limit_val_batches=0, gpus=1, max_epochs=150, progress_bar_refresh_rate=20, default_root_dir='/content/')\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EJNlcutt0xI"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqhknkBZt2ZG",
        "cellView": "form"
      },
      "source": [
        "#@title sort files with predictions (configured for 2 classes, uses efficientnet-b0 as base)\n",
        "import torch\n",
        "import glob\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=2)\n",
        "\n",
        "model_path = '/content/Checkpoint_149_8849_loss_-1.678169_acc_0.855932_D.pth' #@param {type:\"string\"}\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "rootdir = '/content/data' #@param {type:\"string\"}\n",
        "\n",
        "# probably depends on how categories are alphabetically sorted\n",
        "path0 = '/content/output/0' #@param {type:\"string\"}\n",
        "path1 = '/content/output/1' #@param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(path0):\n",
        "    os.makedirs(path0)\n",
        "if not os.path.exists(path1):\n",
        "    os.makedirs(path1)\n",
        "\n",
        "files = glob.glob(rootdir + '/**/*.png', recursive=True)\n",
        "files_jpg = glob.glob(rootdir + '/**/*.jpg', recursive=True)\n",
        "files.extend(files_jpg)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "height_min = 256\n",
        "width_min = 256\n",
        "\n",
        "with torch.no_grad():\n",
        "  for f in tqdm(files):\n",
        "      image = cv2.imread(f)\n",
        "      #image = cv2.resize(image, (256,256))\n",
        "\n",
        "      # resizing to match original training, or detections will be bad\n",
        "      height = image.shape[0]\n",
        "      width = image.shape[1]\n",
        "      if height > height_min and width > width_min:\n",
        "          height_resized = height_min\n",
        "          if width < height:\n",
        "            scale_x = width_min/width\n",
        "            width_resized = width_min\n",
        "            height_resized = scale_x * height\n",
        "          else:\n",
        "            scale_y = height_min/height\n",
        "            height_resized = height_min\n",
        "            width_resized = scale_y * width\n",
        "          image = cv2.resize(image, (int(width_resized), int(height_resized)))\n",
        "      #elif height <= height_min or width <= width_min:\n",
        "      #  break\n",
        "\n",
        "      image = torch.from_numpy(image).unsqueeze(0).permute(0,3,1,2)/255\n",
        "      \n",
        "      image=image.to(device)\n",
        "\n",
        "      y_pred = model(image)\n",
        "\n",
        "      #y_prob = F.softmax(y_pred, dim = -1)\n",
        "      #top_pred = y_prob.argmax(1, keepdim = True)\n",
        "\n",
        "      y_pred = torch.softmax(y_pred, dim=1).float()\n",
        "      top_pred = np.argmax(y_pred.data.cpu().numpy(), axis=1).astype(np.uint8)\n",
        "\n",
        "\n",
        "      if top_pred == 0:\n",
        "        shutil.move(f, os.path.join(path0, os.path.basename(f)))\n",
        "      elif top_pred == 1:\n",
        "        shutil.move(f, os.path.join(path1, os.path.basename(f)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
