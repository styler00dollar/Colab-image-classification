{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9 -  SqueezeNet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skzB79nE-Dco"
      },
      "source": [
        "# Colab-pytorch-image-classification\r\n",
        "Original repo: [bentrevett/pytorch-image-classification](https://github.com/bentrevett/pytorch-image-classification)\r\n",
        "\r\n",
        "[SqueezeNet code](https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py): [pytorch/vision](https://github.com/pytorch/vision)\r\n",
        "\r\n",
        "My fork: [styler00dollar/Colab-image-classification](https://github.com/styler00dollar/Colab-image-classification)\r\n",
        "\r\n",
        "This colab is a combination of [this Colab](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb) and [my other Colab](https://colab.research.google.com/github/styler00dollar/Colab-image-classification/blob/master/5_(small)_ResNet.ipynb) to do SqueezeNet training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UhdgIADdX21"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYHmST_xy3gY"
      },
      "source": [
        "# DATASET CREATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQUSvIJDXrUz",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT74ZbEfdqKM"
      },
      "source": [
        "# copy data somehow\r\n",
        "!mkdir '/content/classification'\r\n",
        "!mkdir '/content/classification/images'\r\n",
        "!cp \"/content/drive/MyDrive/classification_v2.7z\" \"/content/classification/images/classification.7z\"\r\n",
        "%cd /content/classification/images\r\n",
        "!7z x \"classification.7z\"\r\n",
        "!rm -rf /content/classification/images/classification.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnhJlc18BkXp",
        "cellView": "form"
      },
      "source": [
        "#@title dataset creation\n",
        "TRAIN_RATIO = 0.90 #@param {type:\"number\"}\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "#data_dir = os.path.join(ROOT, 'CUB_200_2011')\n",
        "data_dir = '/content/classification' #@param {type:\"string\"}\n",
        "images_dir = os.path.join(data_dir, 'images')\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    shutil.rmtree(train_dir) \n",
        "if os.path.exists(test_dir):\n",
        "    shutil.rmtree(test_dir)\n",
        "    \n",
        "os.makedirs(train_dir)\n",
        "os.makedirs(test_dir)\n",
        "\n",
        "classes = os.listdir(images_dir)\n",
        "\n",
        "for c in classes:\n",
        "    \n",
        "    class_dir = os.path.join(images_dir, c)\n",
        "    \n",
        "    images = os.listdir(class_dir)\n",
        "       \n",
        "    n_train = int(len(images) * TRAIN_RATIO)\n",
        "    \n",
        "    train_images = images[:n_train]\n",
        "    test_images = images[n_train:]\n",
        "    \n",
        "    os.makedirs(os.path.join(train_dir, c), exist_ok = True)\n",
        "    os.makedirs(os.path.join(test_dir, c), exist_ok = True)\n",
        "    \n",
        "    for image in tqdm(train_images):\n",
        "        image_src = os.path.join(class_dir, image)\n",
        "        image_dst = os.path.join(train_dir, c, image) \n",
        "        shutil.copyfile(image_src, image_dst)\n",
        "        \n",
        "    for image in tqdm(test_images):\n",
        "        image_src = os.path.join(class_dir, image)\n",
        "        image_dst = os.path.join(test_dir, c, image) \n",
        "        shutil.copyfile(image_src, image_dst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sENsL4f6y66s"
      },
      "source": [
        "# CALC MEANS & STDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORf1yn3Pw4H",
        "cellView": "form"
      },
      "source": [
        "#@title print means and stds\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "train_data = datasets.ImageFolder(root = train_dir, \n",
        "                                  transform = transforms.ToTensor())\n",
        "\n",
        "means = torch.zeros(3)\n",
        "stds = torch.zeros(3)\n",
        "\n",
        "for img, label in tqdm(train_data):\n",
        "    means += torch.mean(img, dim = (1,2))\n",
        "    stds += torch.std(img, dim = (1,2))\n",
        "\n",
        "means /= len(train_data)\n",
        "stds /= len(train_data)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f'Calculated means: {means}')\n",
        "print(f'Calculated stds: {stds}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFKOEBQazWJc"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NilSkBKhPthJ",
        "cellView": "form"
      },
      "source": [
        "#@title import, seed, transforms, dataloader, functions, plot, model, parameter\n",
        "%cd /content/\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "SEED = 1234 #@param {type:\"number\"}\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "train_dir = '/content/classification/train' #@param {type:\"string\"}\n",
        "test_dir = '/content/classification/test' #@param {type:\"string\"}\n",
        "pretrained_size = 256 #@param {type:\"number\"}\n",
        "pretrained_means = [0.6830, 0.6076, 0.6056] #@param {type:\"raw\"}\n",
        "pretrained_stds= [0.2411, 0.2407, 0.2306] #@param {type:\"raw\"}\n",
        "\n",
        "\n",
        "#https://github.com/mit-han-lab/data-efficient-gans/blob/master/DiffAugment_pytorch.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                           transforms.Resize(pretrained_size),\n",
        "                           transforms.RandomRotation(5),\n",
        "                           transforms.RandomHorizontalFlip(0.5),\n",
        "                           transforms.RandomCrop(pretrained_size, padding = 10),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = pretrained_means, \n",
        "                                                std = pretrained_stds)\n",
        "                       ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.Resize(pretrained_size),\n",
        "                           transforms.CenterCrop(pretrained_size),\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = pretrained_means, \n",
        "                                                std = pretrained_stds)\n",
        "                       ])\n",
        "\n",
        "train_data = datasets.ImageFolder(root = train_dir, \n",
        "                                  transform = train_transforms)\n",
        "\n",
        "test_data = datasets.ImageFolder(root = test_dir, \n",
        "                                 transform = test_transforms)      \n",
        "\n",
        "VALID_RATIO = 0.90 #@param {type:\"number\"}\n",
        "\n",
        "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
        "n_valid_examples = len(train_data) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(train_data, \n",
        "                                           [n_train_examples, n_valid_examples])\n",
        "\n",
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "\n",
        "BATCH_SIZE = 32 #@param {type:\"number\"}\n",
        "\n",
        "train_iterator = data.DataLoader(train_data, \n",
        "                                 shuffle = True, \n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "valid_iterator = data.DataLoader(valid_data, \n",
        "                                 batch_size = BATCH_SIZE)\n",
        "\n",
        "test_iterator = data.DataLoader(test_data, \n",
        "                                batch_size = BATCH_SIZE)\n",
        "\n",
        "def normalize_image(image):\n",
        "    image_min = image.min()\n",
        "    image_max = image.max()\n",
        "    image.clamp_(min = image_min, max = image_max)\n",
        "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
        "    return image    \n",
        "def plot_images(images, labels, classes, normalize = True):\n",
        "\n",
        "    n_images = len(images)\n",
        "\n",
        "    rows = int(np.sqrt(n_images))\n",
        "    cols = int(np.sqrt(n_images))\n",
        "\n",
        "    fig = plt.figure(figsize = (15, 15))\n",
        "\n",
        "    for i in range(rows*cols):\n",
        "\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        \n",
        "        image = images[i]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "        label = classes[labels[i]]\n",
        "        ax.set_title(label)\n",
        "        ax.axis('off')\n",
        "\n",
        "N_IMAGES = 25 #@param {type:\"number\"}\n",
        "\n",
        "images, labels = zip(*[(image, label) for image, label in \n",
        "                           [train_data[i] for i in range(N_IMAGES)]])\n",
        "\n",
        "classes = test_data.classes\n",
        "\n",
        "plot_images(images, labels, classes)\n",
        "\n",
        "def format_label(label):\n",
        "    label = label.split('.')[-1]\n",
        "    label = label.replace('_', ' ')\n",
        "    label = label.title()\n",
        "    label = label.replace(' ', '')\n",
        "    return label\n",
        "\n",
        "test_data.classes = [format_label(c) for c in test_data.classes]\n",
        "\n",
        "classes = test_data.classes\n",
        "\n",
        "plot_images(images, labels, classes)\n",
        "\n",
        "\n",
        "#https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "#from .utils import load_state_dict_from_url\n",
        "from typing import Any\n",
        "\n",
        "#__all__ = ['SqueezeNet', 'squeezenet1_0', 'squeezenet1_1']\n",
        "\n",
        "model_urls = {\n",
        "    '1_0': 'https://download.pytorch.org/models/squeezenet1_0-a815701f.pth',\n",
        "    '1_1': 'https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class Fire(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        squeeze_planes: int,\n",
        "        expand1x1_planes: int,\n",
        "        expand3x3_planes: int\n",
        "    ) -> None:\n",
        "        super(Fire, self).__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n",
        "                                   kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n",
        "                                   kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat([\n",
        "            self.expand1x1_activation(self.expand1x1(x)),\n",
        "            self.expand3x3_activation(self.expand3x3(x))\n",
        "        ], 1)\n",
        "\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        version: str = '1_0',\n",
        "        num_classes: int = 1000\n",
        "    ) -> None:\n",
        "        super(SqueezeNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == '1_0':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == '1_1':\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
        "                             \"1_0 or 1_1 expected\".format(version=version))\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            final_conv,\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n",
        "\n",
        "def _squeezenet(version: str, pretrained: bool, progress: bool, **kwargs: Any) -> SqueezeNet:\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = 'squeezenet' + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> SqueezeNet:\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_0', pretrained, progress, **kwargs)\n",
        "\n",
        "\n",
        "def squeezenet1_1(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> SqueezeNet:\n",
        "    r\"\"\"SqueezeNet 1.1 model from the `official SqueezeNet repo\n",
        "    <https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1>`_.\n",
        "    SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters\n",
        "    than SqueezeNet 1.0, without sacrificing accuracy.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet('1_1', pretrained, progress, **kwargs)\n",
        "\n",
        "\"\"\"\n",
        "#https://github.com/pytorch/vision/blob/master/torchvision/models/utils.py\n",
        "try:\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\"\"\"\n",
        "\n",
        "model_train = '1_0' #@param [\"1_0\", \"1_1\"] {type:\"string\"}\n",
        "if model_train == '1_0':\n",
        "  model = SqueezeNet(num_classes=len(test_data.classes), version='1_0')\n",
        "  #state_dict = load_state_dict_from_url(model_urls[model_train],\n",
        "  #                                            progress=True)\n",
        "  #model.load_state_dict(state_dict)\n",
        "elif model_train == '1_1':\n",
        "  model = SqueezeNet(num_classes=len(test_data.classes), version='1_1')\n",
        "  #state_dict = load_state_dict_from_url(model_urls[model_train],\n",
        "  #                                            progress=True)\n",
        "  #model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "START_LR = 1e-7 #@param {type:\"number\"}\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=START_LR)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "class LRFinder:\n",
        "    def __init__(self, model, optimizer, criterion, device):\n",
        "        \n",
        "        self.optimizer = optimizer\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        \n",
        "        torch.save(model.state_dict(), 'init_params.pt')\n",
        "\n",
        "    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n",
        "                   smooth_f = 0.05, diverge_th = 5):\n",
        "        \n",
        "        lrs = []\n",
        "        losses = []\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
        "        \n",
        "        iterator = IteratorWrapper(iterator)\n",
        "        \n",
        "        for iteration in tqdm(range(num_iter)):\n",
        "\n",
        "            loss = self._train_batch(iterator)\n",
        "\n",
        "            #update lr\n",
        "            lr_scheduler.step()\n",
        "            \n",
        "            lrs.append(lr_scheduler.get_lr()[0])\n",
        "\n",
        "            if iteration > 0:\n",
        "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
        "                \n",
        "            if loss < best_loss:\n",
        "                best_loss = loss\n",
        "\n",
        "            losses.append(loss)\n",
        "            \n",
        "            if loss > diverge_th * best_loss:\n",
        "                print(\"Stopping early, the loss has diverged\")\n",
        "                break\n",
        "                       \n",
        "        #reset model to initial parameters\n",
        "        model.load_state_dict(torch.load('init_params.pt'))\n",
        "                    \n",
        "        return lrs, losses\n",
        "\n",
        "    def _train_batch(self, iterator):\n",
        "        \n",
        "        self.model.train()\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        x, y = iterator.get_batch()\n",
        "        \n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        \n",
        "        y_pred, _ = self.model(x)\n",
        "                \n",
        "        loss = self.criterion(y_pred, y)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "class ExponentialLR(_LRScheduler):\n",
        "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
        "        self.end_lr = end_lr\n",
        "        self.num_iter = num_iter\n",
        "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        curr_iter = self.last_epoch + 1\n",
        "        r = curr_iter / self.num_iter\n",
        "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
        "\n",
        "class IteratorWrapper:\n",
        "    def __init__(self, iterator):\n",
        "        self.iterator = iterator\n",
        "        self._iterator = iter(iterator)\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            inputs, labels = next(self._iterator)\n",
        "        except StopIteration:\n",
        "            self._iterator = iter(self.iterator)\n",
        "            inputs, labels, *_ = next(self._iterator)\n",
        "\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_batch(self):\n",
        "        return next(self)\n",
        "\n",
        "\n",
        "def calculate_topk_accuracy(y_pred, y, k = 5):\n",
        "    with torch.no_grad():\n",
        "        batch_size = y.shape[0]\n",
        "\n",
        "        _, top_pred = y_pred.topk(k=1)\n",
        "        \n",
        "        top_pred = top_pred.t()\n",
        "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
        "        correct_1 = correct[:1].view(-1).float().sum(0, keepdim = True)\n",
        "        #correct_k = correct[:k].view(-1).float().sum(0, keepdim = True)\n",
        "        acc_1 = correct_1 / batch_size\n",
        "        #acc_k = correct_k / batch_size\n",
        "        acc_k = 0\n",
        "    return acc_1, acc_k\n",
        "\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, scheduler, device, current_epoch):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_acc_5 = 0\n",
        "    \n",
        "    model.train()\n",
        "    policy = 'color,translation,cutout' #@param {type:\"string\"}\n",
        "    diffaug_activate = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "    #https://stackoverflow.com/questions/45465031/printing-text-below-tqdm-progress-bar\n",
        "    with tqdm(iterator, position=1, bar_format='{desc}') as desc:\n",
        "\n",
        "      for (x, y) in tqdm(iterator, position=0):\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          if diffaug_activate == False:\n",
        "            y_pred = model(x)\n",
        "          else:\n",
        "            y_pred = model(DiffAugment(x, policy=policy))\n",
        "\n",
        "          loss = criterion(y_pred, y)\n",
        "          \n",
        "          acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
        "\n",
        "          \n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "          \n",
        "          scheduler.step()\n",
        "          \n",
        "          epoch_loss += loss.item()\n",
        "          epoch_acc_1 += acc_1.item()\n",
        "          #epoch_acc_5 += acc_5.item()\n",
        "      \n",
        "      epoch_loss /= len(iterator)\n",
        "      epoch_acc_1 /= len(iterator)\n",
        "      desc.set_description(f'Epoch: {current_epoch+1}')\n",
        "      desc.set_description(f'\\tTrain Loss: {epoch_loss:.3f} | Train Acc @1: {epoch_acc_1*100:6.2f}% | ' \\\n",
        "          f'Train Acc @5: {epoch_acc_5*100:6.2f}%')\n",
        "\n",
        "\n",
        "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc_1 = 0\n",
        "    epoch_acc_5 = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        with tqdm(iterator, position=0, bar_format='{desc}', leave=True) as desc:\n",
        "          for (x, y) in iterator:\n",
        "\n",
        "              x = x.to(device)\n",
        "              y = y.to(device)\n",
        "\n",
        "              y_pred = model(x)\n",
        "\n",
        "              loss = criterion(y_pred, y)\n",
        "\n",
        "              acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n",
        "\n",
        "              epoch_loss += loss.item()\n",
        "              epoch_acc_1 += acc_1.item()\n",
        "              #epoch_acc_5 += acc_5.item()\n",
        "        \n",
        "          epoch_loss /= len(iterator)\n",
        "          epoch_acc_1 /= len(iterator)\n",
        "          #epoch_acc_5 /= len(iterator)\n",
        "          \n",
        "          desc.set_description(f'\\tValid Loss: {epoch_loss:.3f} | Valid Acc @1: {epoch_acc_1*100:6.2f}% | ' \\\n",
        "              f'Valid Acc @5: {epoch_acc_5*100:6.2f}%')\n",
        "    return epoch_loss, epoch_acc_1, epoch_acc_5\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxtlrpfWZk8t",
        "cellView": "form"
      },
      "source": [
        "#@title lr_finder\n",
        "END_LR = 10 #@param {type:\"number\"}\n",
        "NUM_ITER =  100#@param {type:\"number\"} #100\n",
        "\n",
        "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
        "lrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBzT3twzZnAW",
        "cellView": "form"
      },
      "source": [
        "#@title plot_lr_finder\n",
        "def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n",
        "    \n",
        "    if skip_end == 0:\n",
        "        lrs = lrs[skip_start:]\n",
        "        losses = losses[skip_start:]\n",
        "    else:\n",
        "        lrs = lrs[skip_start:-skip_end]\n",
        "        losses = losses[skip_start:-skip_end]\n",
        "    \n",
        "    fig = plt.figure(figsize = (16,8))\n",
        "    ax = fig.add_subplot(1,1,1)\n",
        "    ax.plot(lrs, losses)\n",
        "    ax.set_xscale('log')\n",
        "    ax.set_xlabel('Learning rate')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.grid(True, 'both', 'x')\n",
        "    plt.show()\n",
        "\n",
        "plot_lr_finder(lrs, losses, skip_start = 30, skip_end = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDjS7JvNBkZR",
        "cellView": "form"
      },
      "source": [
        "#@title config\n",
        "FOUND_LR = 2e-4 #@param {type:\"number\"}\n",
        "\"\"\"\n",
        "params = [\n",
        "          {'params': model.conv1.parameters(), 'lr': FOUND_LR / 10},\n",
        "          {'params': model.bn1.parameters(), 'lr': FOUND_LR / 10},\n",
        "          {'params': model.layer1.parameters(), 'lr': FOUND_LR / 8},\n",
        "          {'params': model.layer2.parameters(), 'lr': FOUND_LR / 6},\n",
        "          {'params': model.layer3.parameters(), 'lr': FOUND_LR / 4},\n",
        "          {'params': model.layer4.parameters(), 'lr': FOUND_LR / 2},\n",
        "          {'params': model.fc.parameters()}\n",
        "         ]\n",
        "\n",
        "\"\"\"\n",
        "#optimizer = optim.Adam(params, lr = FOUND_LR)\n",
        "optimizer = optim.Adam(model.parameters(), lr = FOUND_LR)\n",
        "\n",
        "EPOCHS = 25 #@param {type:\"number\"}\n",
        "STEPS_PER_EPOCH = len(train_iterator)\n",
        "TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
        "\n",
        "MAX_LRS = [p['lr'] for p in optimizer.param_groups]\n",
        "\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer,\n",
        "                                    max_lr = MAX_LRS,\n",
        "                                    total_steps = TOTAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvoyX823apEN",
        "cellView": "form"
      },
      "source": [
        "#@title training without topk\n",
        "import time\n",
        "best_valid_loss = float('inf')\n",
        "best_valid_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.monotonic()\n",
        "    \n",
        "    train_loss, train_acc_1, train_acc_5 = train(model, train_iterator, optimizer, criterion, scheduler, device, epoch)\n",
        "    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, valid_iterator, criterion, device)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'best-validation-loss.pt')\n",
        "    if best_valid_accuracy < valid_acc_1:\n",
        "        best_valid_accuracy = valid_acc_1\n",
        "        torch.save(model.state_dict(), 'best-validation-accuracy.pt')\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccBDif9BzHRD"
      },
      "source": [
        "#####################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxb15eSW9VsB"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOla2axMBkZk",
        "cellView": "form"
      },
      "source": [
        "#@title Calc test loss\n",
        "model.load_state_dict(torch.load('best-validation-accuracy.pt'))\n",
        "print(\"best-validation-accuracy.pt\")\n",
        "test_loss, test_acc_1, test_acc_5 = evaluate(model, test_iterator, criterion, device)\n",
        "print(\"-----------------------------\")\n",
        "model.load_state_dict(torch.load('best-validation-loss.pt'))\n",
        "print(\"best-validation-loss.pt\")\n",
        "test_loss, test_acc_1, test_acc_5 = evaluate(model, test_iterator, criterion, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlCBiYxQbi4s",
        "cellView": "form"
      },
      "source": [
        "#@title plot_confusion_matrix\n",
        "def get_predictions(model, iterator):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            y_pred, _ = model(x)\n",
        "\n",
        "            y_prob = F.softmax(y_pred, dim = -1)\n",
        "            top_pred = y_prob.argmax(1, keepdim = True)\n",
        "\n",
        "            images.append(x.cpu())\n",
        "            labels.append(y.cpu())\n",
        "            probs.append(y_prob.cpu())\n",
        "\n",
        "    images = torch.cat(images, dim = 0)\n",
        "    labels = torch.cat(labels, dim = 0)\n",
        "    probs = torch.cat(probs, dim = 0)\n",
        "\n",
        "    return images, labels, probs\n",
        "\n",
        "images, labels, probs = get_predictions(model, test_iterator)\n",
        "pred_labels = torch.argmax(probs, 1)\n",
        "\n",
        "def plot_confusion_matrix(labels, pred_labels, classes):\n",
        "    \n",
        "    fig = plt.figure(figsize = (50, 50));\n",
        "    ax = fig.add_subplot(1, 1, 1);\n",
        "    cm = confusion_matrix(labels, pred_labels);\n",
        "    cm = ConfusionMatrixDisplay(cm, display_labels = classes);\n",
        "    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n",
        "    fig.delaxes(fig.axes[1]) #delete colorbar\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.xlabel('Predicted Label', fontsize = 50)\n",
        "    plt.ylabel('True Label', fontsize = 50)\n",
        "\n",
        "plot_confusion_matrix(labels, pred_labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgSkMA9dbzy8",
        "cellView": "form"
      },
      "source": [
        "#@title plot\r\n",
        "corrects = torch.eq(labels, pred_labels)\r\n",
        "\r\n",
        "incorrect_examples = []\r\n",
        "\r\n",
        "for image, label, prob, correct in zip(images, labels, probs, corrects):\r\n",
        "    if not correct:\r\n",
        "        incorrect_examples.append((image, label, prob))\r\n",
        "\r\n",
        "incorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)\r\n",
        "\r\n",
        "def plot_most_incorrect(incorrect, classes, n_images, normalize = True):\r\n",
        "\r\n",
        "    rows = int(np.sqrt(n_images))\r\n",
        "    cols = int(np.sqrt(n_images))\r\n",
        "\r\n",
        "    fig = plt.figure(figsize = (25, 20))\r\n",
        "\r\n",
        "    for i in range(rows*cols):\r\n",
        "\r\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\r\n",
        "        \r\n",
        "        image, true_label, probs = incorrect[i]\r\n",
        "        image = image.permute(1, 2, 0)\r\n",
        "        true_prob = probs[true_label]\r\n",
        "        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\r\n",
        "        true_class = classes[true_label]\r\n",
        "        incorrect_class = classes[incorrect_label]\r\n",
        "\r\n",
        "        if normalize:\r\n",
        "            image = normalize_image(image)\r\n",
        "\r\n",
        "        ax.imshow(image.cpu().numpy())\r\n",
        "        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\r\n",
        "                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\r\n",
        "        ax.axis('off')\r\n",
        "        \r\n",
        "    fig.subplots_adjust(hspace=0.4)\r\n",
        "\r\n",
        "N_IMAGES = 36\r\n",
        "\r\n",
        "plot_most_incorrect(incorrect_examples, classes, N_IMAGES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5UzPHw29QVi",
        "cellView": "form"
      },
      "source": [
        "#@title plot_representations\n",
        "def get_representations(model, iterator):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    outputs = []\n",
        "    intermediates = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for (x, y) in iterator:\n",
        "\n",
        "            x = x.to(device)\n",
        "\n",
        "            y_pred, _ = model(x)\n",
        "\n",
        "            outputs.append(y_pred.cpu())\n",
        "            labels.append(y)\n",
        "        \n",
        "    outputs = torch.cat(outputs, dim = 0)\n",
        "    labels = torch.cat(labels, dim = 0)\n",
        "\n",
        "    return outputs, labels\n",
        "\n",
        "outputs, labels = get_representations(model, train_iterator)\n",
        "\n",
        "def get_pca(data, n_components = 2):\n",
        "    pca = decomposition.PCA()\n",
        "    pca.n_components = n_components\n",
        "    pca_data = pca.fit_transform(data)\n",
        "    return pca_data\n",
        "    \n",
        "def plot_representations(data, labels, classes, n_images = None):\n",
        "            \n",
        "    if n_images is not None:\n",
        "        data = data[:n_images]\n",
        "        labels = labels[:n_images]\n",
        "                \n",
        "    fig = plt.figure(figsize = (15, 15))\n",
        "    ax = fig.add_subplot(111)\n",
        "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'hsv')\n",
        "    #handles, _ = scatter.legend_elements(num = None)\n",
        "    #legend = plt.legend(handles = handles, labels = classes)\n",
        "\n",
        "output_pca_data = get_pca(outputs)\n",
        "plot_representations(output_pca_data, labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz6yxrS29i9D",
        "cellView": "form"
      },
      "source": [
        "#@title get_tsne\n",
        "def get_tsne(data, n_components = 2, n_images = None):\n",
        "    \n",
        "    if n_images is not None:\n",
        "        data = data[:n_images]\n",
        "        \n",
        "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
        "    tsne_data = tsne.fit_transform(data)\n",
        "    return tsne_data\n",
        "\n",
        "output_tsne_data = get_tsne(outputs)\n",
        "plot_representations(output_tsne_data, labels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDqM5kXc9mU7",
        "cellView": "form"
      },
      "source": [
        "#@title plot_filtered_images\n",
        "def plot_filtered_images(images, filters, n_filters = None, normalize = True):\n",
        "\n",
        "    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n",
        "    filters = filters.cpu()\n",
        "\n",
        "    if n_filters is not None:\n",
        "        filters = filters[:n_filters]\n",
        "\n",
        "    n_images = images.shape[0]\n",
        "    n_filters = filters.shape[0]\n",
        "\n",
        "    filtered_images = F.conv2d(images, filters)\n",
        "\n",
        "    fig = plt.figure(figsize = (30, 30))\n",
        "\n",
        "    for i in range(n_images):\n",
        "\n",
        "        image = images[i]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n",
        "        ax.imshow(image.permute(1,2,0).numpy())\n",
        "        ax.set_title('Original')\n",
        "        ax.axis('off')\n",
        "\n",
        "        for j in range(n_filters):\n",
        "            image = filtered_images[i][j]\n",
        "\n",
        "            if normalize:\n",
        "                image = normalize_image(image)\n",
        "\n",
        "            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n",
        "            ax.imshow(image.numpy(), cmap = 'bone')\n",
        "            ax.set_title(f'Filter {j+1}')\n",
        "            ax.axis('off');\n",
        "\n",
        "    fig.subplots_adjust(hspace = -0.7)\n",
        "\n",
        "N_IMAGES = 5\n",
        "N_FILTERS = 7\n",
        "\n",
        "images = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\n",
        "filters = model.conv1.weight.data\n",
        "\n",
        "plot_filtered_images(images, filters, N_FILTERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOGoonSkxVNk",
        "cellView": "form"
      },
      "source": [
        "#@title plot_filters\n",
        "#filters = model.conv1.weight.data\n",
        "\n",
        "def plot_filters(filters, normalize = True):\n",
        "\n",
        "    filters = filters.cpu()\n",
        "\n",
        "    n_filters = filters.shape[0]\n",
        "\n",
        "    rows = int(np.sqrt(n_filters))\n",
        "    cols = int(np.sqrt(n_filters))\n",
        "\n",
        "    fig = plt.figure(figsize = (30, 15))\n",
        "\n",
        "    for i in range(rows*cols):\n",
        "\n",
        "        image = filters[i]\n",
        "\n",
        "        if normalize:\n",
        "            image = normalize_image(image)\n",
        "\n",
        "        ax = fig.add_subplot(rows, cols, i+1)\n",
        "        ax.imshow(image.permute(1, 2, 0))\n",
        "        ax.axis('off')\n",
        "        \n",
        "    fig.subplots_adjust(wspace = -0.9)\n",
        "\n",
        "plot_filters(filters)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}